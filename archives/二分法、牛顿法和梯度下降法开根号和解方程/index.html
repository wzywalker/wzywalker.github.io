<!DOCTYPE HTML>
<html lang="english">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,AlanDecode,Galileo,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Galileo" />
    <link rel="alternate" type="application/rss+xml" title="Maverick &raquo; RSS 2.0" href="/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="Maverick &raquo; ATOM 1.0" href="/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/galileo-8d8763e752.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-182e5a8868.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700&display=swap">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/b3f8e527f1f4c5f1418ca21962b619c2.json"
        }
    </script>
    
<title>二分法、牛顿法和梯度下降法开根号和解方程 - Maverick</title>
<meta name="author" content="walker" />
<meta name="description" content="二分法" />
<meta property="og:title" content="二分法、牛顿法和梯度下降法开根号和解方程 - Maverick" />
<meta property="og:description" content="二分法" />
<meta property="og:site_name" content="Maverick" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/archives/%E4%BA%8C%E5%88%86%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%BC%80%E6%A0%B9%E5%8F%B7%E5%92%8C%E8%A7%A3%E6%96%B9%E7%A8%8B/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2021-01-14T00:00:00-00.00" />
<meta name="twitter:title" content="二分法、牛顿法和梯度下降法开根号和解方程 - Maverick" />
<meta name="twitter:description" content="二分法" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
    </head>
    
    <body>
        
        <div class="container">
            <header id="ga-header">
                <div first>
                    <aside id="ga-brand">
                        <h1 class="brand"><a class="no-style" href="/">Maverick</a></h1>
                        <p>This is Maverick, Theme Galileo.</p>
                    </aside>
                </div>
                <div second id="ga-nav">
                    <nav class="navs">
                        <ul><li><a class="ga-highlight" href="/" target="_self">Home</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/archives/" target="_self">Archives</a></li><span class="separator">·</span><li><a class="ga-highlight" href="/about/" target="_self">About</a></li><span class="separator">·</span><li><a href="#" target="_self" class="search-form-input ga-highlight">Search</a></li></ul>
                    </nav>
                </div>
            </header>
            <div class="wrapper">
                
<main>    
    <section class="ga-section ga-content">
        <article class="yue">
            <h1 class="ga-post_title">二分法、牛顿法和梯度下降法开根号和解方程</h1>
            <span class="ga-post_meta ga-mono">
                <span>walker</span>
                <time>
                    2021-01-14
                </time>
                
                in <a no-style class="category" href="/category/posts/">
                    posts
                </a>
                
                
                <span class="leancloud_visitors" 
                    id="/archives/%E4%BA%8C%E5%88%86%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%BC%80%E6%A0%B9%E5%8F%B7%E5%92%8C%E8%A7%A3%E6%96%B9%E7%A8%8B/" 
                    data-flag-title="二分法、牛顿法和梯度下降法开根号和解方程"> · <i class="leancloud-visitors-count"></i> Views</span>
                
            </span>
            <div class="ga-content_body">
                <h2>二分法</h2>
<p>二分法的思路是每次排除一半样本的试错方法，把样本一分为二（A和B），那么目标值不在A就在B里，不断缩小范围。</p><p>就像在玩一个猜价格的游戏，通过告诉你猜高了还是低了，你总能猜到正确价格一样，设计好一个计算差值的函数能大体上判断出你下一轮尝试的值应该在前一半还是后一半，总能迭代到足够接近的结果。</p><p>对于求平方根来说，我们没什么过多的设计，直接对中值取平方，高了就取小的一半，低了就取大的一半，实测小的数字是没问题的，这里仅仅用来演示思路。</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">binary_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>         <span class="c1"># quit flag</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">n</span>
    <span class="n">mid</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># 值过大则尝试小的一半，否则就尝试大的一半，修改边界值即可</span>
        <span class="k">if</span> <span class="n">diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">mid</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;estimated:</span><span class="se">\t</span><span class="si">{</span><span class="n">binary_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">): </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
<p>output:</p>
<pre><code>estimated:	0.9999999999708962, 	 sqrt(1): 	 1.0
estimated:	1.4142135623842478, 	 sqrt(2): 	 1.4142135623730951
estimated:	1.7320508075645193, 	 sqrt(3): 	 1.7320508075688772
estimated:	2.0000000000000000, 	 sqrt(4): 	 2.0
estimated:	2.2360679775010794, 	 sqrt(5): 	 2.23606797749979
estimated:	2.4494897427794060, 	 sqrt(6): 	 2.449489742783178
estimated:	2.6457513110653963, 	 sqrt(7): 	 2.6457513110645907
estimated:	2.8284271247393917, 	 sqrt(8): 	 2.8284271247461903
estimated:	2.9999999999890860, 	 sqrt(9): 	 3.0
estimated:	3.1622776601579970, 	 sqrt(10): 	 3.1622776601683795
</code></pre>
<h2>牛顿法</h2>
<p>我就算不画图也能把这事说清楚。</p><p>牛顿法用的是斜率的思想，对$f(x)=0$，选一个足够接近目标值($x$)的点($x_0$)，计算其切线与X轴的交点($x_1$），这个交点$x_1$往往比$x_0$更接近$x$，数次迭代后肯定越来越接近目标值$x$。</p><ol>
<li>问题转化成一个求函数上任一点($x_n$)的切线与X轴的交点($x_{n+1}$)的问题(我们假设<code>n+1</code>在<code>n</code>的左边，即向左来逼近$x_0$)</li>
<li>令$\Delta x = x_n - x_{n+1}, \Delta y = f(x_n) - f(x_{n+1})$，则$f'(x_n) = 该点斜率 = \frac{\Delta y}{\Delta x}$</li>
<li>展开:$f'(x_n) = \frac{f(x_n) - f(x_{n +1})}{x_n - x_{n+1}}$</li>
<li>$\because f(x_{n+1})=0\ \Rightarrow x_{n +1} = x_n - \frac{f(x_n)}{f'(x_n)}$</li>
<li>至此，我们用$x_n$推出了一个离$x_0$更近的点：$x_{n+1}$</li>
<li>如此往复则可以推出足够精度的解。</li>
</ol>
<p>而求任意正整数$a$的平方根，</p><ol>
<li>函数就变成了 $g(x) = a, \Rightarrow g(x) = x^2$，</li>
<li>那么我们有: $f(x) = g(x) - a = 0 = x^2 - a = 0$</li>
<li>$f'(x) = 2x$</li>
<li>$f(x),f'(x)$都有了，就能代入上面得到的公式：$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$了</li>
<li>得到$x_{n+1} = x_n - \frac{x_n^2-a}{2x_n}$</li>
</ol>
<p>现在可以写代码了，不断去迭代，求下一个$x_{n+1}$:</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">newton_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x_n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>             <span class="c1"># quit flag</span>

    <span class="n">f_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">n</span>   <span class="c1"># f(x)=x^2 - a</span>
    <span class="n">df_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">a</span>       <span class="c1"># derivative of f(x)</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_n</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">x_n</span> <span class="o">=</span> <span class="n">x_next</span><span class="p">(</span><span class="n">x_n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_n</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)</span><span class="se">\t</span><span class="si">{</span><span class="n">newton_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sqrt(1)	1.000000000000001
sqrt(2)	1.4142135623746899
sqrt(3)	1.7320508075688772
sqrt(4)	2.0
sqrt(5)	2.23606797749979
sqrt(6)	2.4494897427831788
sqrt(7)	2.6457513110646933
sqrt(8)	2.8284271247493797
sqrt(9)	3.0
</code></pre>
<h2>梯度下降法</h2>
<p>梯度下降法的数学原理是$f(x_1,x_2,\dots$)的gradient（$\nabla f$）就是其最陡爬升方向（<code>steepest ascent</code>）。</p><p>可以拿这个当成结论，也可以去感性认识，而要去证明的话，网上有数不清的教程，在花书(《Deep Learning深度学习》)和可汗学院里，都是用的<code>directional derivate</code>来解释（证明）的，即”<strong>自定义方向</strong>上的瞬时变化率“，也是我认为在如果有多变量微积分的基础下，比较容易让人接受且简单直白的解释：
<figure style="flex: 34.96774193548387" ><img loading="lazy" width="1084" height="1550" src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/archives/assets/e7889d3178ce0131e3ac9dcc816d03c3.png" /></figure></p><ul>
<li>$\overset{\rightarrow}{v} \cdot \nabla f = |\overset{\rightarrow}{v}|\cdot|\nabla f|\cdot cos\theta$</li>
<li>$\overset{\rightarrow}{v}$ 就是指的任意方向，如果是x, y等方向，那就是普通的偏导了。</li>
<li>显然上式当$\theta = 0$时拥有最大值，即$\overset{\rightarrow}{v}$指向的是$\nabla f$的方向，那就是梯度的方向了</li>
<li>所以梯度方向就是<code>爬升最陡峭</code>的方向</li>
</ul>
<p>在一元方程里，”梯度“就是过某点的斜率（<code>slope</code>)，或者说函数的导数（<code>derivative</code>）。</p><p>我们要到局部最小值，显然就应该向相向的方向走。并且由于越接近目标值（谷底），斜率越小，所以即使我们选择一个固定的步长（<code>learning rate</code>），也是会有一个越来越小的步进值去逼近极值，而无需刻意去调整步长。</p><p>以上是思路，只是要注意它$\color{red}{并不是作用到要求的函数本身}$上去的，而是精心设计的<code>loss</code>，或者说<code>diff</code>、<code>error</code>函数。</p><p>其实它跟前面的<code>二分法</code>很类似，就是不断指导里应该在哪个区间里去尝试下一个x值，再来结果与真值的差异（而<code>牛顿法</code>则是直接朝着直值去逼近，并不是在“尝试“）。</p><p>二分法里我随便设计了一个判断loss的函数（即中值的平方减真值），而梯度下降里不能那么随便了，它需要是一个连续的函数（即可微分），还要至少拥有局部极小值：</p><p>我们令$e(x)$表示不同的x取值下与目标值$Y$的差的平方（损失函数<em>loss</em>），既然是一个简单二次函数，就能求极值，且它的最小值意味着当x值为该值时估算原函数$f(x)=Y$的<strong>误差最小</strong>，有：</p><p>$e(x) = \frac{1}{2}(f(x) - Y)^2$  (1/2的作用仅仅是为了取导数时消除常数项，简化计算)<br />
$e'(x) = (f(x) - Y) \cdot f'(x) = \Delta y \cdot f'(x)\quad \color{green}{\Leftarrow Chain\ Rule}$<br />
$\Delta x = e'(x) \cdot lr = \Delta y \cdot f'(x) \cdot lr\ \color{red}{\Leftarrow这里得到了更新x的依据}$<br />
$x_{n+1} = x_n - \Delta x = x_n - \Delta y \cdot f'(x) \cdot lr \Leftarrow 公式有了$</p><p>这时可以写代码了</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span>       <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">2</span>       <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.01</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>       <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">df_dx</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">a</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span><span class="n">n</span>
    <span class="n">e_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="mf">0.5</span>     <span class="c1"># funcon of loss</span>
    <span class="n">de_dx</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">df_dx</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># derivative of loss</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span> <span class="p">:</span> <span class="n">de_dx</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="n">count</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">delta_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">count</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sqrt(</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">): </span><span class="si">{</span><span class="n">gradient_sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">次&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sqrt(1): (0.9999999999519603, 593)次
sqrt(2): (1.4142135623377403, 285)次
sqrt(3): (1.7320508075423036, 181)次
sqrt(4): (2.0, 0)次
sqrt(5): (2.236067977522142, 103)次
sqrt(6): (2.449489742798969, 87)次
sqrt(7): (2.645751311082885, 73)次
sqrt(8): (2.828427124761154, 63)次
sqrt(9): (3.00000000001166, 55)次
</code></pre>
<hr />
<p><strong>Bonus</strong></p><h2>梯度下降解二次方程</h2>
<ul>
<li>求解方程：$(x_1 - 3)^2 + (x_2 + 4)^2 = 0$的根</li>
</ul>
<p>$f(x) = (x_1 - 3)^2 + (x_2 + 4)^2 = 0$</p><p>$e(x) = \frac{1}{2}(f(x)-Y)^2$</p><p>$\frac{\partial}{\partial x_1}e(x)=(f(x)-Y)\cdot(f(x) -Y)'
= (f(x)-Y)\cdot\frac{\partial}{\partial x_1}((x_1 - 3)^2 + (x_2 + 4)^2-Y)$</p><p>$\therefore
\begin{cases}
\frac{\partial}{\partial x_1}e(x)=\Delta y \cdot 2(x_1 - 3) \
\frac{\partial}{\partial x_2}e(x)=\Delta y \cdot 2(x_2 + 4)
\end{cases}
$</p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_f</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span>  <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>        <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.01</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-4</span>        <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="p">(</span><span class="n">x1</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x2</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">dfx1</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">dfx2</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span>
    <span class="n">e_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="mf">0.5</span>     <span class="c1"># cost function</span>
    <span class="n">dedx1</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dfx1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>   <span class="c1"># partial derivative of loss \</span>
    <span class="n">dedx2</span>   <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dfx2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>   <span class="c1"># with Chain Rule</span>
    <span class="n">delt_x1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">dedx1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">delt_x2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="p">:</span> <span class="n">dedx2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="n">count</span>   <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">x1</span> <span class="o">-=</span> <span class="n">delt_x1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">-=</span> <span class="n">delt_x2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">count</span>

<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">gradient_f</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">a </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1"></span>
<span class="s1">b </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1"> </span>
<span class="s1">f(a, b) = </span><span class="si">{</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s1"></span>
<span class="s1">count </span><span class="se">\t</span><span class="s1">= </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>a 	= 2.9967765158140387
b 	= -3.9905337923806563 
f(a, b) = 9.999993698966316e-05
count 	= 249990
</code></pre>
<p>之所以做两个练习， 就是因为第一个是故意把过程写得非常详细，如果直接套公式的话，而不是把每一步推导都写成代码，可以简单很多（其实就是最后一步的结果）:$\Delta x = \Delta y \cdot f'(x) \cdot lr$</p><h2>梯度下降解反三角函数</h2>
<ul>
<li>求解arcsin(x)，在$x = 0.5$和$x = \frac{\sqrt{3}}{2}$的值</li>
</ul>
<p>即估算两个x值，令$f(x)=sin(x)=0.5$和$f(x)=sin(x)=\frac{\sqrt{3}}{2}$<br />
这次不推导了，套一次公式吧$\Delta x = \Delta y \cdot f'(x) \cdot lr$</p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">arcsin</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span>       <span class="o">=</span> <span class="mi">1</span>           <span class="c1"># first try</span>
    <span class="n">lr</span>      <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># learning rate</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>        <span class="c1"># quit flag</span>

    <span class="n">f_x</span>     <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">delta_y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">delta_y</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">delta_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;sin(</span><span class="si">{</span><span class="n">arcsin</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">}</span><span class="s1">) ≈ 0.5</span>
<span class="s1">sin(</span><span class="si">{</span><span class="n">arcsin</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">) ≈ sqrt(3)/2</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
<p>output</p>
<pre><code>sin(30.000000638736502) ≈ 0.5
sin(59.999998857570986) ≈ sqrt(3)/2

</code></pre>

            </div>
        </article>
        <div id="ga-tags">
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/%E4%BA%8C%E5%88%86%E6%B3%95/">#二分法</a>
    </span>
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/%E7%89%9B%E9%A1%BF%E6%B3%95/">#牛顿法</a>
    </span>
    
    <span class="ga-tag">
        <a class="ga-highlight" href="/tag/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">#梯度下降</a>
    </span>
    
</div>
    </section>

    
<section id="ga-content_pager">

    <div class="next">
        <a class="ga-highlight" href="/archives/add_subplots%E6%96%B9%E6%B3%95%E4%BC%A0%E9%80%92%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0/">add_subplots方法传递额外参数</a>
        <p class="yue">用matplotlib绘制3D图，最快的用法ax = plt.axes(projection=&#39;3d&#39;)</p>
    </div>


    <div class="prev">
        <h3>No More</h3>
        <p class="yue">This is the oldest one.</p>
    </div>

</section>


    
        <script>
            var initValine = function () {
                new Valine({"enable": true, "el": "#vcomments", "appId": "7tP92LoqK2cggW61DvJmWBo0-gzGzoHsz", "appKey": "iQCtrtlr8eKrQllM03GMESMJ", "visitor": true, "recordIP": true});
            }
        </script>
        <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
        <div id="vcomments"></div>
    

</main>

                <footer class="ga-mono" id="ga-footer">
                    <section>
                        <span id="ga-uptime"></span>
                        <span class="brand">Maverick</span>
                    </section>
                    <section>
                        <p class="copyright">
                            <span>Copyright © 2022 AlanDecode</span>
                            <span>Powered by <a no-style href="https://github.com/AlanDecode/Maverick" target="_blank">Maverick & Galileo</a></span>
                        </p>
                        <div class="copyright">
                            <span class="footer-addon">
                                
                            </span>
                            <nav class="social-links">
                                <ul><li><a class="no-style" title="Twitter" href="https://twitter.com/walkerwzy" target="_blank"><i class="gi gi-twitter"></i>Twitter</a></li><span class="separator">·</span><li><a class="no-style" title="GitHub" href="https://github.com/walkerwzy" target="_blank"><i class="gi gi-github"></i>GitHub</a></li><span class="separator">·</span><li><a class="no-style" title="Weibo" href="https://weibo.com/1071696872" target="_blank"><i class="gi gi-weibo"></i>Weibo</a></li></ul>
                            </nav>
                        </div>
                    </section>
                    <script>
                        var site_build_date = "2019-12-06T12:00+08:00"
                    </script>
                    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/galileo-7c8cea54ab.js"></script>
                </footer>
            </div>
        </div>
    </div>

    <!--katex-->
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/katex.min.js"></script>
    <script>
    mathOpts = {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/wzywalker/wzywalker.github.io@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    
    </body>
</html>